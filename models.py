""" Encoder and decoder models """

import torch
import torch.nn as nn
import torchvision.models as models
from torch.nn.utils.rnn import pack_padded_sequence

class EncoderCNN(nn.Module):
    def __init__(self,embed_size):
        """Load the pretrained ResNet-152 and replace top fc layer."""

        super(EncoderCNN, self).__init__()
        resnet = models.resnet152(pretrained=True)

         # Keep all layers except the last one
        layers = list(resnet.children())[:-1]   
        # unpack the layers and create a new Sequential
        self.resnet = nn.Sequential(*layers)
        self.linear = nn.Linear(resnet.fc.in_features,embed_size)

        self.bn = nn.BatchNorm1d(embed_size,momentum=0.01)

        self.init_weights()

    def init_weights():
        """ initialize the weights"""
            self.linear.weight.data.normal_(0.0,0.02)
            
            self.linear.bias.data.fill_(0)

        
    def forward(self, images):
        """Extract feature vectors from input images."""

        features = self.resnet(images)
        features = Variable(feature.data)
        features = features.view (features.size(0),-1)
        features = self.bn(self.linear(features))
        



        # QUESTION 1.2
        # TODO


        return features


class DecoderRNN(nn.Module):
    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):
        """Set the hyper-parameters and build the layers."""
        super(DecoderRNN, self).__init__()
        # We want a specific output size, which is the size of our embedding, so
        # we feed our extracted features from the last fc layer (dimensions 1 x 4096)
        # into a Linear layer to resize
        self.resize = nn.Linear(2048, embed_size)
        # Batch normalisation helps to speed up training
        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)

        # What is an embedding layer?
        self.embed = nn.Embedding(vocab_size, embed_size)
        



        # QUESTION 1.3 DecoderRNN - define this layer 
        # TODO
        # self.rnn 



        self.linear = nn.Linear(hidden_size, vocab_size)
        
        self.max_seq_length = max_seq_length
        

    def forward(self, features, captions, lengths):
        """Decode image feature vectors and generates captions."""
        embeddings = self.embed(captions)
        im_features = self.resize(features)
        im_features = self.bn(im_features)
        embeddings = torch.cat((im_features.unsqueeze(1), embeddings), 1)
        
        # What is "packing" a sequence?
        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) 
        hiddens, _ = self.rnn(packed)
        outputs = self.linear(hiddens[0])
        return outputs
    

    def sample(self, features, states=None):
        """Generate captions for given image features using greedy search."""

        sampled_ids = []
        
        inputs = self.bn(self.resize(features.unsqueeze[1]))
        for i in range(self.max_seq_length):
            hiddens, states = self.rnn(inputs, states)          # hiddens: (batch_size, 1, hidden_size)
            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)
            predicted = outputs.max(1)                     # predicted: (batch_size)
            sampled_ids.append(predicted)
            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)
            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)
        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)
        return sampled_ids